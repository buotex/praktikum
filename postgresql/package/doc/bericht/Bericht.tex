\documentclass{DEarticle}
\usepackage{lmodern}
\usepackage{cite}
\usepackage{hyperref}
\usepackage{amssymb}
\title{Praktikumsbericht}
\author{Buote Xu}

\begin{document}
\maketitle
\begin{abstract}
Eine Zusammenfassung der implementierten Methoden 
im Bereich ``Graph-Edit-Distance'', 
ihre Anwendung auf generierten sowie echten Daten und 
ein Ausblick für weitere Entwicklungsmöglichkeiten.
\end{abstract}

\section{Einführung}
Aufgabe des Praktikums war, ein oder mehrere Algorithmen aus \cite{comp} zu implementieren, um ihre Tauglichkeit auf
einigen Graphtypen zu überprüfen, dabei wurde vor allem die Methode basierend auf \emph{Hidden Markov Models} evaluiert.
Es hat sich herausgestellt, dass die vorgeschlagene Distanz sich sehr instabil gegenüber kleinen
Veränderungen an den Daten zeigt, weswegen auf Basis von \cite{unscented} und \cite{randomwalk} eine neue
Methode \ref{newmethod} entwickelt wurde. Die beiden implementierten Algorithmen werden im Folgenden beschrieben und
evaluiert.
Die verwendete Programmiersprache ist C++.


\section{EDH-Based GED}
\subsection{Beschreibung}
\cite[EDH-Based GED]{comp}
Dieser Algorithmus ist in zwei Phasen aufgeteilt und dient dazu, Graphen anhand ihrer Kanten zu beschreiben.
\begin{enumerate}
\item Erstelle ein Histogramm, um die verschiedenen Kanten eines Graphen zu kategorisieren. Die von den Autoren
verwendete Methode nutzt hierfür die bekannten kartesischen Raumwinkel. Dabei ist es prinzipiell egal (und so auch
implementiert), ob der Winkel ein Label der Kante an sich ist oder über die Position ihrer Knoten bestimmt werden kann.
\item Finde die günstigste Zuordnung zweier Histogramme zueinander; hierfür wurde nur der zweidimensionale Fall,
beschrieben in
\cite[Appendix A]{sift}
implementiert, da durch die Periodizität der Winkel $(360^{\circ} = 0^{\circ})$ ein sinnvolles, rotations-invariantes
Matching berechenbar ist.
\end{enumerate}
\subsection{Implementierung}
Um eine möglichst generische Benutzung zu ermöglichen, wurde die Boost Graph Library \cite{boostgraph} verwendet, dabei
ist ein sehr generischer und bedauerlicherweise auch sehr schwer lesbarer Code entstanden.
Features:
\begin{itemize}
\item Unterstützung für beliebige Boost-Graphs, wenn für die Knoten jeweils eine ``location'' definiert wird.
\item In der Lage, die Ähnlichkeit/Distanz zwischen zwei eindimensionalen Histogrammen (also zweidimensionalen Daten) zu
berechnen
\item Es ist möglich, auch andere Methoden zur Berechnung von Winkeln hinzuzufügen (z.B. für Geodaten), wenn man die
passende Klasse schreibt.
\end{itemize}
\subsection{Kommentar}
Der Algorithmus hat einige gute Eigenschaften, so ist er invariant gegenüber Rotation, wegen der Normalisierung auch
gegen Streckung, dies könnte theoretisch bei der Analyse von Buchstaben/Schrift von Vorteil sein, dazu ist er zumindest
in zwei Dimensionen sehr flott: Die Laufzeit beträgt (O(E)), also linear in der Kantenmenge. 

Eine Spiegelung der
Daten hingegen führt dazu, dass sich das Histogram umdreht - genau hier existiert ein Problem des Ansatzes; ein matching
zwischen zwei Histogrammen wie vorgeschlagen hat wenige Freiheiten.

So ist es nur möglich, Winkel linear einander zuzuordnen - wenn also z.B. die Winkel $(0_A^{\circ} = 30_B^{\circ})$ der
Graphen $A$ und $B$ zueinander passen, so wird damit automatisch auch festgelegt, dass $(40_A^{\circ} = 70_B^{\circ})$
gilt - die Histogramme werden also quasi bestmöglich zueinander \emph{verschoben}, was bei der genannten Spiegelung oder
auch einer Zerrung eben nicht erfolgreich sein kann.

Ein weiteres Problem ist die Skalierbarkeit in höhere Dimensionen: Schon bei 3 Dimensionen hat man es mit
2-dimensionalen Histogrammen zu tun, die größtenteils leer sind (je nach Auflösung der Histogramme), so dass sinnvolle
Zuordnungen schwer gefunden werden können.
Aus diesen Gründen wurde das Hauptaugenmerk auf den nächsten Algorithmus gelegt.

\section{HMM-Based GED}
\cite[HMM-Based GED]{comp}
Auch dies ist ein zweigeteilter Algorithmus, HMM zur Modellierung eines Graphen und Distanzen um diese Modelle
miteinander zu vergleichen.
\subsection{HMM}
\subsubsection{Beschreibung}
\emph{HMMs} oder genauer \emph{Hidden Markov Models} werden oft in der Sprach- und Bildverarbeitung genutzt, dennoch
eine kurze Erklärung, für Details ist \cite{rabin} zu empfehlen.

Für einen Informatiker ist die naheliegendste Struktur wohl eine \emph{Finite State Machine}, die in jedem
\emph{Zeit-}schritt mit einer gewissen Wahrscheinlichkeit ein Symbol ausgibt. 

Es ist möglich, damit mehrere Problemstellungen zu bewältigen; in dem vorliegenden Fall ist das Ziel folgendes: 

Angenommen, ein \emph{Hidden Markov Model} hat eine Folge von Symbolen (also z.B. Atome in einem Protein, oder
abstrakter, Punkte im $\mathbb{R}^3$ ausgegeben - welches waren die wahrscheinlichsten Parameter? (Mit welchem
\emph{State} wurde begonnen, wie sind die Transitionswahrscheinlichkeiten, mit welcher Wahrscheinlichkeit gibt
\emph{State} $3$ das Symbol $x$ aus?

Da es im $\mathbb{R}^3$ eine unendliche Anzahl an möglichen Symbolen gibt (vgl. hierzu ein Model, das nur die
Symbole \emph{Sonne} und \emph{Regen} kennt, und das Wetter in Mannheim grob beschreiben soll), ist es notwendig für
jeden State eine \emph{Wahrscheinlichkeitsverteilung} zu finden.

Dies geschieht über \emph{Gaussian Mixture Models}, die eine gewichtete Ansammlung von \emph{Gaussian
Models}, also Glockenkurven-Verteilungen sind. So wird also jedem \emph{State} genau ein \emph{Gaussian Mixture Model}
zugewiesen.
Um vernünftige Startwerte für ein mögliches Modell zu erzeugen, wird gemäß \cite{comp} ein \cite{emgmm}
\emph{Expectation Maximization}-Algorithmus angewandt, der für eine gegebene Punktmenge das wahrscheinlichste
\emph{Gaussian Mixture Model} berechnet - dabei ist zu bemerken, dass das perfekte Modell nicht gefunden werden kann und
somit mögliche Lösungen iterativ erzeugt werden bis sie sich nicht mehr merklich verbessern.


Nach Erzeugung von HMMs für zwei Graphen gibt es verschiedene \cite{randomwalk}, \cite{comp} Ähnlichkeits-/Distanzmaße,
die später vorgestellt werden.


\paragraph{Fragen:}
\begin{itemize}
\item \textbf{Welches sind die States in einem Graphen?} Die Autoren in \cite{comp} schlagen vor, einen \emph{KMeans}
anzuwenden, um dann jedem einzelnen Cluster einen State \label{states} zuzuordnen. Dies ist ein sinnvoller Ansatz, um die \emph{GMMs}
lokal einzuschränken.
\item \textbf{Inwiefern spielen Graphkanten eine Rolle?} Die einzige Möglichkeit, diese mit einzubeziehen, ist während
des \emph{Clustering}; so kann z.B. der \cite{chinese} \emph{Chinese Whisper}-Algorithmus verwendet werden, der
vor allem auf dicht besetzten Graphen zu sinnvollen Ergebnissen führt.
\item \textbf{Wie ist die zeitliche Ordnung von stationären, also z.B. Proteindaten?} Hier kommen wir zu einem Problem des ursprünglichen
Algorithmus in \cite{comp}: Durch Vertauschung der Reihenfolge von wenigen Punkten erhält man unendliche Distanzen, da
\emph{HMMs} primär für die Beschreibung von zeitabhängigen Daten geeignet sind, es gibt aber auch für stationäre Daten
bessere Distanzmaße die stabiler gegenüber solchen Änderungen sind.
\end{itemize}

\subsubsection{Implementierung}
Da es sich vornehmlich um Matrix-Operationen handelt, wurde \emph{Armadillo} \cite{armadillo} eingesetzt, eine
hinreichend schnelle Bibliothek, die zusammen mit \emph{Atlas} oder \emph{Lapack} die nötige Lineare Algebra
bereitstellt.
Daten-Punkte werden hierbei als Spalten von Armadillo-Matrizen übergeben, für \emph{pdb}-Dateien ist unter den \emph{examples}
ein Parser beigelegt, der die Atome aus ebendiesen extrahiert und passende Matrizen erzeugt.

Aufgrund mangelnder Open-Source-Implementierungen von \emph{HMMs}, die auf \emph{GMMs} basieren, wurden diese im Zuge
des Praktikums in Anlehnung an \cite{emgmm} und \cite{rabin} geschrieben und über \emph{Doxygen} dokumentiert.

\paragraph{Features:}
\begin{itemize}
\item \emph{Geschwindigkeit:} Im Laufe der Entwicklung haben sich einige Stellen zur Optimierung aufgetan, sodass nun
für die Parametersuche eine HMM über den \emph{Baum-Welch}-Algorithmus zusätzlich eine Methode angeboten wird, die durch
Caching einiger Wahrscheinlichkeiten sowie günstiger Matrix-Multiplikationen sich 5-10x schneller als die naive, wenn auch
speichergünstigere Variante zeigt.
Die Laufzeitkomplexität ist, aufgrund eines sehr schnellen Konvergenzverhaltens der iterativen Methoden (~10-20
Schritte) als linear in der Anzahl der Datenpunkte zu betrachten.
\item \emph{Kompatibilität:} Er werden Objekte im $\mathbb{R}^n$ unterstützt, da die unterliegenden GMMs nur hierfür
sinnvoll definiert sind. Es wäre denkbar, für diskrete Symbole (z.B. Angebot eines Online-Versenders) eine passende
Template-Version anzubieten, dies ist aber nicht Inhalt dieser Arbeit.
\item \emph{Numerische Stabilität:} Wie in \cite{rabin} beschrieben, wurden in dieser Implementierung normierte
Wahrscheinlichkeiten verwendet, da ansonsten die Multiplikation von kleinen Zahlen zur völligen Auslöschung führen
würde. Dabei ist zu beachten, dass im ursprünglichen Werk Fehler enthalten waren welche in \cite{erratumr} behoben wurden.
\end{itemize}


\subsection{Distanzen und Ähnlichkeiten}

Um eine Angabe über die Ähnlichkeit zweier berechneter HMMs zu machen, wird ein Maß verwendet, um die enthaltenen
Wahrscheinlichkeitsverteilungen der beiden miteinander zu vergleichen - wenn sich diese ähneln, so auch die damit
beschriebenen Daten.
\subsubsection{KL-Divergenz}
Ein häufig verwendetes Maß für zwei Verteilungen $P, Q$ ist die \emph{Kullback-Leibler}-Divergenz: 
$$KL(P,Q) = \int \! P(x) \log {P(x) \over Q(x)} \, \mathrm{d} x$$
Diese hat jedoch im konkreten Fall einige Probleme,
da sie über ein Integral definiert ist, welches für kompliziertere Verteilungen nur näherungsweise berechnet werden
kann.
Hier ist zu bemerken, dass bei der Modellierung eines Datensatzes aufgrund der vorherigen Einteilung in wenig-überlappende Cluster (mit KMeans z.B.) 
\emph{GMMs} entstehen, die sich stark voneinander unterscheiden. Ein Datenpunkt kann somit oftmals nur von einer
einzigen GMM (und damit einem \emph{State}, vgl. \ref{states}) erzeugt werden, was bei der Distanzberechnung zu
Problemen führt.

\emph{Bemerkung:} Die KLD zwischen zwei unabhängigen Verteilungen ist quasi unendlich aufgrund des $\log$, wenn ${P(x)
\over Q(x)} \approx 0 $

\paragraph {KLD-Berechnung nach \cite{comp}}
Die dort vorgeschlagene Näherung hat genau das Problem, dass nicht alle States $(S_i)_i$, $(T_j)_j$ zweier HMMs miteinander
verglichen werden, sondern nur $S_1$ mit $T_1$, $S_2$ mit $T_2$ etc., so kann es bei einer ungünstigen Nummerierung dazu
kommen, dass die resultierende Distanz unendlich ist - es ist eher erforderlich, die jeweils nächstliegenden States
einander zuzuordnen. 
Weiterhin zeigt sich der Algorithmus absolut instabil gegenüber kleinen Änderungen an der Inputreihenfolge, was ihn z.B.
für Proteinanalyse vollkommen untauglich macht. 

Überhaupt kriegt er Schwierigkeiten, wenn ``zu viele'' Daten vorhanden
sind, so dass die enstehenden Verteilungen zu einseitig sind (z.B. wenn die Wahrscheinlichkeit, im State 4 zu starten
bei $100\%$ liegt).

Es hat sich herausgestellt, dass der umgekehrte Ansatz, die Ähnlichkeit statt der Distanz/Divergenz
zu berechnen, vielversprechender ist, wie sich im nächsten Algorithmus zeigt.

\paragraph {RandomWalk \cite{randomwalk}}
\label{newmethod}
Dieser Ansatz führt prinzipiell sehr ähnliche Berechnungen durch - da aber ein entgegengesetztes Maß berechnet werden
soll (also Ähnlichkeit statt Distanz), kann $e^{-d}$ für eine Distanz $d$ berechnet werden, um zusammen
mit der KLD-Näherung in \cite{unscented} Werte zu erhalten, die zwischen $0$ und $1$ liegen.

Somit kann man für die States $(S_i)_i$, $(T_j)_j$  eine Matrix mit $i * j$ Einträgen erstellen, die die jeweiligen
Ähnlichkeiten beschreibt.

Das resultierende Ähnlichkeitsmaß zweier HMMs hat folgende Eigenschaften:
\begin{itemize}
\item \emph{Wertebereich:} Ursprünglich zwischen 0 und 1, um aber sinnvollere Ergebnisse zu erhalten fließen im Gegensatz
zu \cite{randomwalk} nun die absoluten Ähnlichkeiten zwischen den States mit ein, was bei extrem ähnlichen HMMs (also
z.B. wenn sie übereinstimmen) zu geringfügig höheren Ergebnissen wie 1.3 führen können.
\item \emph{Geschwindigkeit:} Linear bezüglich der Anzahl an verwendeten Gaussian Models, also unerheblich im Vergleich
zur HMM-Berechnung
\item \emph{Interpretierbarkeit:} Aus den Zwischenergebnissen kann man z.B. über die \emph{Ungarische Methode} ein
bestmögliches Mapping zwischen den States erhalten, dies wurde aus Zeitgründen jedoch nicht mehr implementiert.
\item \emph{Invarianz:} Basierend auf der Idee von stationären Wahrscheinlichkeiten in \cite{randomwalk}
(diese geben an, welche States eher besucht werden) ist es möglich und sinnvoll, etwaige Veränderungen der Ausgangsdaten
zu \emph{reverse engineeren}. So ist eine Methode enthalten, die mit Hilfe von Clustermittelpunkten lineare
Transformationen durchführt, um zwei HMMs soweit wie möglich aneinander anzupassen.
Dadurch wird der Algorithmus stabil (also $100\%$ Übereinstimmung zwischen den veränderten Daten) gegenüber Spiegelung, Streckung und Drehung, jedoch nicht Verschiebungen - 
Daten sollten deshalb vor der HMM-Berechnung \emph{zentriert} werden.
\item \emph{Abhängigkeit von der Inputreihenfolge:} Diese ist immernoch gegeben, jedoch nicht so stark ausgeprägt wie
beim ursprünglichen Ansatz, sodass z.B. zwischen einem Protein und dessen gemischten Äquivalent noch eine wesentlich
höhere Ähnlichkeit besteht als einem anderem Protein.

\end{itemize}

\subsection{Kommentar}
Am Ende ist ein schneller, stabiler, interpretierbarer Algorithmus entstanden, der so natürlich auch für ein
Klassifizierungen verwendet werden kann.
Wegen dem Einsatz verschiedener Wahrscheinlichkeitsverteilungen und statistischen Vorgängen ist er für kleine Graphen
(also in der Größenordnung bis ~100 Knoten) ungeeignet, dafür skaliert er aber auch noch gut mit größeren Mengen. 
Weitere Anwendungsideen wären Text- und Sprachanalysen, wo HMMs häufig verwendet werden, dafür müssten allerdings noch
die unterliegenden GMMs angepasst werden.







\begin{thebibliography}{1}
\bibitem{comp} X.-B. Gao, B. Xiao et. al. A Comparative Study of Three Graph Edit Distance Algorithms. Foundations on Computational Intelligence (Edited by A. Abraham, Aboul-Ella Hassanien et al.), ISBN: 978-3-642-01535-9, Springer, Vol. 5, SCI 205, pp. 223-242, 2009. 
\bibitem{rabin} L. R. Rabiner (1989). "A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition," Proceedings of the IEEE, vol 77, no 2, 257--287.
\bibitem{unscented} Goldberger, Gordon, Greenspan. An Efficient Image Similarity Measure Based on Approximations of KL-Divergence Between Two Gaussian Mixtures, Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on In Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on (2003), pp. 487-493 vol.1, doi:10.1109/ICCV.2003.1238387 
\bibitem{randomwalk} A Novel Low-Complexity HMM Similarity Measure. Sayed Mohammad Ebrahim Sahraeian, Student Member, IEEE, and Byung-Jun Yoon, Member, IEEE
\bibitem{emgmm} Unsupersived Learning of Finite Mixture Models. Mario A.T. Figueiredo and Anil K. Jain
\bibitem{armadillo} Conrad Sanderson. Armadillo: An Open Source C++ Linear Algebra Library for Fast Prototyping and Computationally Intensive Experiments. Technical Report, NICTA, 2010.
\bibitem{esbtl} Loriot S, Cazals F, Bernauer J: ESBTL: efficient PDB parser and data structure for the structural and geometric analysis of biological macromolecules. Bioinformatics 2010, 26(8):1127-1128. PMID: 20185407
\bibitem{sift} A Linear Time Histogram Metric for Improved SIFT Matching. Ofir Pele and Michael Werman. 
\bibitem{boostgraph} The Boost Graph Library. Jeremy Siek, Lie-Quan Lee, Andrew Lumsdaine.
\bibitem{chinese} Chinese Whispers - an Efficient Graph Clustering Algorithm and its Application to Natural Language
Processing Problems. Christian Biemann 
\bibitem{erratumr} An Erratum for ``A Tutorial on Hidden Markov Models and Selected Applications in Speech
Recognition''. Ali Rahimi 
\end{thebibliography}



\end{document}
